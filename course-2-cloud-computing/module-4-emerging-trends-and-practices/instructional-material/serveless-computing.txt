[MUSIC] Serverless is an approach to computing
that offloads responsibility for common infrastructure management
tasks such as scaling, scheduling, patching, and provisioning application
stacks to cloud providers allowing developers to focus their time and
effort on the code and business logic specific to
their applications or process. Serverless doesn't mean there are no
servers, only that the management of the underlying physical or virtual
servers is removed from their users. The serverless computing environment
allocates resources as needed for the applications. Let's look at some key attributes that
distinguish serverless computing from other compute models. The serverless model requires
no provisioning of servers, installation of application stacks and
software or operation of the infrastructure
by the developer. Serverless computing runs code only
on-demand on a per request basis, scaling transparently with
the number of requests being served. Serverless enables end users to
pay only for resources being used, never paying for idle capacity, which
is unlike virtual servers on the cloud, where end users pay for VMs as long
as they are running, even if idle. Effectively serverless abstracts
the infrastructure away from developers, code is executed as individual functions, where each function runs
inside a stateless container. No prior execution context is
required to serve a request, and with each new request,
a new instance of the function is invoked. Let's look at a scenario,
you could, for example, have a serverless platform between
the front end of your website and your storage layer running individual
functions, the serverless app could be translating text files and storing
it in a cloud-based storage service. Using the front end of your website,
you send text files to a serverless app. The app creates translations in different
languages and then stores these translated files in a Cloud Storage and
sends you their links back to you. Some of the key serverless
computing services today include IBM Cloud functions,
which is based on Apache OpenWhisk, AWS Lambda and Microsoft Azure functions. It is important to note that
serverless may not be the best fit for all applications or scenarios. You need to evaluate
application characteristics and ensure that the application is aligned
to serverless architecture patterns. Applications that qualify for
a serverless architecture include some of the following characteristics,
short-running stateless functions, seconds or minutes, seasonal workloads
with varying off peak and peaks, production volumetric data that shows too
much idle time, event-based processing or asynchronous request processing for
implementing use cases. Microservices that can be built
as functions that are stateless. Serverless architectures are well
suited for use cases around, data and event processing, IoT,
microservices, and mobile back ends. Given its inherent and automatic scaling,
rapid provisioning and a pricing model that does not change for idle time,
supporting microservices architecture has become one of the most common use
cases of serverless computing today. Serverless is well suited to working
with structured text, audio, image, and video data around
tasks such as data enrichment, transformation, validation,
and cleansing, PDF processing, audio normalization, thumbnail generation,
and video transcoding. Parallel tasks such as data search and
processing and genome processing are also well suited
to be run on a serverless runtime. Serverless is also well suited for working
with all sorts of data stream ingestions, including business data streams,
IoT sensor data, log data, and financial market data. And finally, let's look at some challenges
worth considering about serverless. Serverless workloads are designed to scale
up and down in response to workload. But for workloads characterized
by long-running processes, managing a traditional server environment
might be simpler and more cost effective. The serverless application architecture
can be vendor dependent, and so there is a potential for
vendor lock-in, particularly involving platform
capabilities such as authentication, scaling, monitoring, or
configuration management. Because serverless architectures scale
up and down in response to workload, they also sometimes need to start up
from zero to serve a new request. For certain applications, this delay isn't
much of an impact, but for something like a low latency financial application,
this delay wouldn't be acceptable. [MUSIC]